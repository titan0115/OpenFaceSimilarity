#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
"""

import os
import torch
import argparse
from models.mobilefacenet import MobileFaceNet, count_parameters
from models.losses import ArcFace, CosFace
from utils.dataset import create_real_dataloader
from utils.metrics import compute_verification_metrics, plot_verification_metrics, print_metrics_summary
from inference import FaceComparator


def demo_model_architecture():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏"""
    print("=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ú–û–î–ï–õ–ò")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = MobileFaceNet(embedding_size=128, input_size=112)
    print(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: MobileFaceNet")
    print(f"–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: 112x112x3")
    print(f"–†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: 128")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {count_parameters(model):,}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ö–æ–¥
    print("\n–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å:")
    x = torch.randn(4, 3, 112, 112)
    with torch.no_grad():
        embeddings = model(x)
    
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä: {x.shape}")
    print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä: {embeddings.shape}")
    print(f"–ù–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {torch.norm(embeddings, p=2, dim=1)}")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")


def demo_loss_functions():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–π"""
    print("\n" + "=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø LOSS –§–£–ù–ö–¶–ò–ô")
    print("=" * 60)
    
    batch_size = 8
    embedding_size = 128
    num_classes = 100
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    embeddings = torch.nn.functional.normalize(torch.randn(batch_size, embedding_size), p=2, dim=1)
    labels = torch.randint(0, num_classes, (batch_size,))
    
    print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {batch_size} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, {num_classes} –∫–ª–∞—Å—Å–æ–≤")
    
    # ArcFace
    print("\nüî• ArcFace Loss:")
    arcface = ArcFace(embedding_size, num_classes, margin=0.5, scale=64.0)
    arcface_output = arcface(embeddings, labels)
    arcface_loss = torch.nn.functional.cross_entropy(arcface_output, labels)
    print(f"   –í—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {arcface_output.shape}")
    print(f"   Loss: {arcface_loss.item():.4f}")
    
    # CosFace
    print("\nüéØ CosFace Loss:")
    cosface = CosFace(embedding_size, num_classes, margin=0.35, scale=64.0)
    cosface_output = cosface(embeddings, labels)
    cosface_loss = torch.nn.functional.cross_entropy(cosface_output, labels)
    print(f"   –í—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {cosface_output.shape}")
    print(f"   Loss: {cosface_loss.item():.4f}")
    
    print("‚úÖ Loss —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")



def demo_training_simulation():
    """–°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    print("\n" + "=" * 60)
    print("–°–ò–ú–£–õ–Ø–¶–ò–Ø –ü–†–û–¶–ï–°–°–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    embedding_size = 128
    num_classes = 10
    batch_size = 4
    use_amp = torch.cuda.is_available()  # –í–∫–ª—é—á–∞–µ–º AMP —Ç–æ–ª—å–∫–æ –Ω–∞ GPU
    
    # –ú–æ–¥–µ–ª—å –∏ loss
    model = MobileFaceNet(embedding_size=embedding_size)
    loss_fn = ArcFace(embedding_size, num_classes)
    optimizer = torch.optim.Adam([
        {'params': model.parameters()},
        {'params': loss_fn.parameters()}
    ], lr=1e-3)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AMP
    if use_amp:
        from torch.cuda.amp import autocast, GradScaler
        scaler = GradScaler()
        print(f"‚úÖ AMP –≤–∫–ª—é—á–µ–Ω")
    else:
        scaler = None
        print(f"‚ö†Ô∏è AMP –æ—Ç–∫–ª—é—á–µ–Ω (–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ CPU)")
    
    print(f"–ú–æ–¥–µ–ª—å: MobileFaceNet ({count_parameters(model):,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    print(f"Loss: ArcFace")
    print(f"–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam")
    
    # –°–∏–º—É–ª—è—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    print("\n–°–∏–º—É–ª—è—Ü–∏—è 3 —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è:")
    model.train()
    loss_fn.train()
    
    for step in range(3):
        # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        images = torch.randn(batch_size, 3, 112, 112)
        labels = torch.randint(0, num_classes, (batch_size,))
        
        optimizer.zero_grad()
        
        if use_amp:
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å AMP
            with autocast():
                embeddings = model(images)
                logits = loss_fn(embeddings, labels)
                loss = torch.nn.functional.cross_entropy(logits, labels)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —Å AMP
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            # –û–±—ã—á–Ω—ã–π –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            embeddings = model(images)
            logits = loss_fn(embeddings, labels)
            loss = torch.nn.functional.cross_entropy(logits, labels)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            optimizer.step()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
        predictions = torch.argmax(logits, dim=1)
        accuracy = (predictions == labels).float().mean()
        
        print(f"   –®–∞–≥ {step+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy.item():.4f}")
    
    print("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")


def demo_inference():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    print("\n" + "=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ò–ù–§–ï–†–ï–ù–°–ê")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ–º–æ
    model = MobileFaceNet(embedding_size=128)
    model.eval()
    
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∫—Ç–∏–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    import numpy as np
    image1 = np.random.randint(0, 255, (112, 112, 3), dtype=np.uint8)
    image2 = np.random.randint(0, 255, (112, 112, 3), dtype=np.uint8)
    image3 = image1.copy()  # –ò–¥–µ–Ω—Ç–∏—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    def get_embedding(image):
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        with torch.no_grad():
            x = transform(image).unsqueeze(0)
            embedding = model(x)
            return embedding.numpy().flatten()
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    emb1 = get_embedding(image1)
    emb2 = get_embedding(image2)
    emb3 = get_embedding(image3)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    sim_1_2 = cosine_similarity(emb1, emb2)
    sim_1_3 = cosine_similarity(emb1, emb3)
    
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ1 ‚Üî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ2: {sim_1_2:.4f}")
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ1 ‚Üî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ3 (–∏–¥–µ–Ω—Ç–∏—á–Ω–æ–µ): {sim_1_3:.4f}")
    
    threshold = 0.6
    print(f"\n–° –ø–æ—Ä–æ–≥–æ–º {threshold}:")
    print(f"   image1 –∏ image2: {'–°–û–í–ü–ê–î–ï–ù–ò–ï' if sim_1_2 > threshold else '–ù–ï –°–û–í–ü–ê–î–ê–ï–¢'}")
    print(f"   image1 –∏ image3: {'–°–û–í–ü–ê–î–ï–ù–ò–ï' if sim_1_3 > threshold else '–ù–ï –°–û–í–ü–ê–î–ê–ï–¢'}")
    
    print("‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")


def demo_metrics():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""
    print("\n" + "=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç—Ä–∏–∫
    import torch.nn.functional as F
    
    batch_size = 50
    embedding_size = 128
    
    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings1 = F.normalize(torch.randn(batch_size, embedding_size), p=2, dim=1)
    embeddings2 = F.normalize(torch.randn(batch_size, embedding_size), p=2, dim=1)
    
    # –ú–µ—Ç–∫–∏ (50% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
    labels = torch.randint(0, 2, (batch_size,))
    
    # –î–ª—è —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –ø–∞—Ä –¥–µ–ª–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–º–∏
    for i in range(batch_size):
        if labels[i] == 1:
            noise = torch.randn_like(embeddings1[i]) * 0.2
            embeddings2[i] = F.normalize(embeddings1[i] + noise, p=2, dim=0)
    
    print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {batch_size} –ø–∞—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –ø–∞—Ä: {labels.sum().item()}")
    print(f"–†–∞–∑–ª–∏—á–Ω—ã—Ö –ø–∞—Ä: {(1 - labels).sum().item()}")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    from utils.metrics import compute_verification_metrics, print_metrics_summary
    
    print("\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
    metrics = compute_verification_metrics(embeddings1, embeddings2, labels)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print_metrics_summary(metrics)
    
    print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")


def main():
    parser = argparse.ArgumentParser(description='–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü')
    parser.add_argument('--demo', type=str, default='all',
                       choices=['all', 'model', 'loss', 'training', 'inference', 'metrics'],
                       help='–ö–∞–∫—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –∑–∞–ø—É—Å—Ç–∏—Ç—å')
    
    args = parser.parse_args()
    
    print("üé≠ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –õ–ò–¶")
    print("–ü—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –∑–∞–¥–∞–Ω–∏—é")
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    
    if args.demo in ['all', 'model']:
        demo_model_architecture()
    
    if args.demo in ['all', 'loss']:
        demo_loss_functions()
    

    
    if args.demo in ['all', 'training']:
        demo_training_simulation()
    
    if args.demo in ['all', 'inference']:
        demo_inference()
    
    if args.demo in ['all', 'metrics']:
        demo_metrics()
    
    print("\n" + "=" * 60)
    print("üéâ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 60)
    print("–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞:")
    print("‚úÖ MobileFaceNet - –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞")
    print("‚úÖ ArcFace/CosFace - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏")
    print("‚úÖ –†–∞–±–æ—Ç–∞ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print("‚úÖ –ü–æ–ª–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è")
    print("‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª–∏—Ü")
    print("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
    print("\n–î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è: python train.py")
    print("–î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: python inference.py --model_path checkpoints/best_checkpoint.pth --reference img1.jpg --test img2.jpg")


if __name__ == "__main__":
    main()