#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AMP (Automatic Mixed Precision) –≤ –ø—Ä–æ–µ–∫—Ç–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü
"""

import torch
from config import Config, get_fast_config, get_production_config
from train import FaceRecognitionTrainer

def demo_amp_config():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AMP —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    print("=" * 60)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê–°–¢–†–û–ô–ö–ò AMP")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å AMP
    config = Config()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AMP –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    config.training.use_amp = True
    config.training.amp_opt_level = "O1"  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏
    config.training.amp_loss_scale = None  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    # –ü–µ—á–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–∫
    print(f"AMP –≤–∫–ª—é—á–µ–Ω: {config.training.use_amp}")
    print(f"–£—Ä–æ–≤–µ–Ω—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {config.training.amp_opt_level}")
    print(f"–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ loss: {config.training.amp_loss_scale}")
    
    # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å AMP
    print("\n--- –ë—ã—Å—Ç—Ä–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---")
    fast_config = get_fast_config()
    print(f"AMP: {fast_config.training.use_amp}")
    
    print("\n--- –ü—Ä–æ–¥–∞–∫—à–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---")
    prod_config = get_production_config()
    print(f"AMP: {prod_config.training.use_amp}")
    
    return config

def demo_amp_levels():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π AMP –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    print("\n" + "=" * 60)
    print("–£–†–û–í–ù–ò AMP –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
    print("=" * 60)
    
    levels = {
        "O0": "FP32 training (–±–µ–∑ AMP)",
        "O1": "Conservative Mixed Precision (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π)",
        "O2": "Fast Mixed Precision",
        "O3": "FP16 training (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π)"
    }
    
    for level, description in levels.items():
        print(f"{level}: {description}")
    
    print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("- O1: –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é")
    print("- O2: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å, –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ loss_scale")
    print("- O0: –û—Ç–∫–ª—é—á–∞–µ—Ç AMP –ø–æ–ª–Ω–æ—Å—Ç—å—é")
    print("- O3: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π, –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞")

def demo_amp_benefits():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ AMP"""
    print("\n" + "=" * 60)
    print("–ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê AMP")
    print("=" * 60)
    
    benefits = [
        "üöÄ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–æ 1.5-2x –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU",
        "üíæ –°–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 50%",
        "üìà –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö batch_size",
        "‚ö° –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π",
        "üõ°Ô∏è –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ loss scaling",
        "üîß –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞"
    ]
    
    for benefit in benefits:
        print(benefit)
    
    print("\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:")
    print("- GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Tensor Cores (RTX 20xx+, V100+)")
    print("- PyTorch 1.6+ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
    print("- CUDA 10.1+")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AMP –≤ –ø—Ä–æ–µ–∫—Ç–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
    config = demo_amp_config()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    demo_amp_levels()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤
    demo_amp_benefits()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
    print("\n" + "=" * 60)
    print("–ü–†–û–í–ï–†–ö–ê –°–ò–°–¢–ï–ú–´")
    print("=" * 60)
    
    cuda_available = torch.cuda.is_available()
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda_available}")
    
    if cuda_available:
        print(f"–í–µ—Ä—Å–∏—è CUDA: {torch.version.cuda}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
        print(f"–ù–∞–∑–≤–∞–Ω–∏–µ GPU: {torch.cuda.get_device_name(0)}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ AMP
        try:
            from torch.cuda.amp import autocast, GradScaler
            print("‚úÖ AMP –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        except ImportError:
            print("‚ùå AMP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ PyTorch")
    else:
        print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - AMP –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–µ–Ω")
    
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–† –ó–ê–ü–£–°–ö–ê –° AMP")
    print("=" * 60)
    print("python train.py --use_amp --amp_opt_level O1 --batch_size 128")
    print("python train.py --use_amp --amp_opt_level O2 --batch_size 256")

if __name__ == "__main__":
    main()